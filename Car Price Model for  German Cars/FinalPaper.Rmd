---
title: "FinalPaper"
output:
  html_document: default
  pdf_document: default
date: "2022-12-07"
---

```{r setup, warning=FALSE, message=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Import Packages
suppressWarnings({
suppressMessages(library(tidyr))
suppressMessages(library(dplyr))
suppressMessages(library(stringr))
suppressMessages(library(reshape2))
suppressMessages(library(gdata))
suppressMessages(library(lattice))
suppressMessages(library(data.table))
suppressMessages(library(ggplot2))
suppressMessages(library(ggridges))
suppressMessages(library(treemapify))
suppressMessages(library(leaps))
suppressMessages(library(lmtest))
suppressMessages(library(MASS))
suppressMessages(library(faraway))
suppressMessages(library(scales))
})
#Import Data
audi = read.csv("Data/audi.csv")
bmw = read.csv("Data/bmw.csv")
mercedes = read.csv("Data/mercedes_benz.csv")
porsche = read.csv("Data/porsche.csv")
volkswagen = read.csv("Data/volkswagen.csv")
car.ori = rbind(audi,bmw, mercedes, porsche, volkswagen)
car = car.ori

#Data Cleaning Function
car.clean = function(car) {
car[,3] = as.numeric(str_remove(substring(car[,3], 2),","))
car[,4] = as.numeric(str_remove(str_remove(car[,4],",")," mi."))
car = na.omit(car)
car.col.2 = car[,2]
car.col.2 = as.data.frame(car.col.2)
car.col.2.temp = car.col.2 %>%
    mutate(Year = str_extract(car.col.2, "^[^\\s]+"),
           Brand = str_extract(car.col.2, "[\\s]+([^\\s]+)"),
           Make = str_remove(car.col.2, '\\w+\\s\\w+\\s')) %>%
    dplyr::select(-car.col.2)
car = cbind(car.col.2.temp,car)
car = car[,-c(4,5)] 
car$Brand = trim(car$Brand)
car
}

#Audi
audi.clean = car.clean(audi)
audi.clean <- transform(audi.clean, Type = ifelse(grepl("Premium", Make), "Premium", ifelse(grepl("Prestige", Make), "Prestige", "Other")))
#BMW
bmw.clean = car.clean(bmw)
bmw.clean <- transform(bmw.clean, Type = ifelse(grepl("xDrive", Make), "xDrive", ifelse(grepl("sDrive", Make), "sDrive", "Other")))
bmw.clean <- transform(bmw.clean, Style = ifelse(grepl("X3", Make), "X3", ifelse(grepl("X4", Make), "X4",  ifelse(grepl("X5", Make), "X5", ifelse(grepl("X6", Make),
"X6",  ifelse(grepl("X7", Make), "X7", "Other"))))))
#MB
mercedes.clean = car.clean(mercedes)
mercedes.clean <- transform(mercedes.clean, Type = ifelse(grepl("4MATIC", Make), "4MATIC", "Other"))
mercedes.clean <- transform(mercedes.clean, Drive = ifelse(grepl("Base", Make), "Base", "Not Base"))
mercedes.clean <- transform(mercedes.clean, Style = ifelse(grepl("S-", Make) | grepl("-S", Make), "S class", ifelse(grepl("C-", Make) | grepl("-C", Make), "C class",
ifelse(grepl("E-", Make) | grepl("-E", Make), "E class", ifelse(grepl("G-", Make) | grepl("-G", Make), "G class", "Other")))))
#Porsche
porsche.clean = car.clean(porsche)
porsche.clean <- transform(porsche.clean, Style = ifelse(grepl("Cayenne", Make), "Cayenne", ifelse(grepl("Panamera", Make), "Panamera", ifelse(grepl("911", Make),
"911", ifelse(grepl("Carrera", Make), "Carrera", ifelse(grepl("Cayman", Make), "Cayman", ifelse(grepl("Macan", Make), "Macan", ifelse(grepl("Boxster", Make),
"Boxster", "Other"))))))))
#VW
volkswagen.clean = car.clean(volkswagen)
volkswagen.clean <- transform(volkswagen.clean, Style = ifelse(grepl("Atlas", Make), "Atlas", ifelse(grepl("Beetle", Make), "Beetle", ifelse(grepl("Golf", Make), "Golf", ifelse(grepl("Jetta", Make), "Jetta", ifelse(grepl("Touareg", Make), "Touareg", ifelse(grepl("Passat", Make), "Passat", ifelse(grepl("Tiguan", Make), "Tiguan", "Other"))))))))
#All
all.clean = car.clean(car)

#All
all.clean <-transform(all.clean,Years_Old = 2022 - as.numeric(Year))
#Audi
audi.clean <-transform(audi.clean,Years_Old = 2022 - as.numeric(Year))
audi.clean <- transform(audi.clean, Type = ifelse(grepl("Premium Plus", Make), "Premium_Plus", ifelse(grepl("Premium", Make), "Premium", ifelse(grepl("Prestige", Make), "Prestige", "Other"))))
audi.clean <- transform(audi.clean, Class = toupper(substr(audi.clean$Make[], 1,1)))
#BMW
bmw.clean <-transform(bmw.clean,Years_Old = 2022 - as.numeric(Year))
#MB
mercedes.clean <-transform(mercedes.clean,Years_Old = 2022 - as.numeric(Year))
#Porsche
porsche.clean <-transform(porsche.clean,Years_Old = 2022 - as.numeric(Year))
#VW
volkswagen.clean <-transform(volkswagen.clean,Years_Old = 2022 - as.numeric(Year))
```

# Description/Introduction
## Running Project
We have two major files to run. One is the DataCleaning.Rmd and the other is ShinyR.R. The former shows the different visualizations and models while the latter displays the our Shiny application. The Presentation.Rmd also outputs our final presentation.

## Purpose
Our team has a significant interest in cars: the faster and more luxurious the better. Therefore, it would make sense that our keen interest was in German cars which are known to be the best-engineered cars in the world. Obviously, though, we are all students and on a tight budget that makes it difficult to afford such luxurious vehicles. Still, there are likely some German cars that may be just as fast and nice but cheaper in price. Therefore, our team aimed at finding what effects the price of different German cars. Specifically, we were looking for specific factors that effect the price in hopes of finding some specific model or set of models that are available and fit our budget and driving needs. Below, we perform such analysis in an effort to find our dream cars while still being able to afford staying in school. In addition to that, we noticed that such a problem really applies to the general American public as a whole. Specifically, Covid-19 has significantly disrupted supply chains and the availability of different car parts, ad, thus, different cars. Not only has the price of new cars from different dealerships increased significantly over rece years, but also the price of rentals through services liek AVIS, Hertz, and Budget have seen significant increases at multiples of 5 times as high as they were just a year or two before Covid-19 started. Therefore, we found our data analysis crucial to those Americans that still want to travel in luxury at an affordable price.

# Data Retrieval
We decided to retrieve used car data from **www.cars.com** and focus on 5 German brands including Audi, BMW, Mercedes, Porsche and Volkswagen. In addition, we filter only cars listed within 200 miles around Champaign (ZIP: 61820) to make it relates to us who study at UIUC. Thus, we come up with API **https&#58;//www.cars.com/shopping/results/list_price_max=&makes[]=volkswagen&maximum_distance=200&models[]=&page=1&page_size=100&stock_type=used&zip=61820** which we will use for data scraping. In order to retrieve all the data, we use **repeat** loop and increase **page** until no data is returned. Then, we change makes and rerun the code. Technically, we use library **rvest** to get HTML data and point to specific HTML elements which contain required information. The data from this process has 3 fields including Detail, Price, and Mileage. While Price and Mileage can be used easily, Detail contains year, model and sub-model which is very specific to brands so we will need data manipulation technique in the next step.

# R Shiny

To make users better interacting with the data and customize the data visualization based on their needs, we created the ShinyR. In the ShinyR, we mainly have three sections: overall plot, statistics summary, and depreciation trend graph. For the overall plot section, users can choose the specific car brand, car make, and year they want look into, and then the plot for price versus mileage will come up. Users can either choose to click on the data points or brush through an area to get more specific information behind the data. For the statistics summary section, users will get an overview of the minimum, average, mean, and max price and mileage for different brand and make to have a general idea of where the price falls into. As for the depreciation trend graph, it clearly shows how the price of a car will decrease as the age of the car increase. The smaller the slope of the graph indicates a better value-maintain and a better investment choice. Lastly, we also created a download button, enable users to download the data and have a further analysis based on their needs.

## Exploratory Data Analysis

We began our work by analyzing the differences between brands. Below, we have a violin plot which gives a general idea about the distribution of the price based on the car brand. From that plot, we learn a lot about how the brand effects the price. Specifically, we get a more clear idea about how the price data is distributed across our different car brands. We see that Porsche and Mercedes have some specific models that are likely very high-end and much more expensive. Also, Audi likely has some models that are not quite as luxurious but still far from average. Meanwhile, BMW and Volkswagen have almost no such cars.  We see that there is a point where all brands have a very highly distributed amount of variables that is at nearly the same spot for all three brands except Mercedes and Volkswagen. For Mercedes we see both a lower and higher peak instead, and for Volkswagen we see that peak in the distribution at a slightly lower price point.

```{r, fig.width=6, fig.height=4}
ggplot(all.clean,aes(x=Brand,y=Price)) + 
  geom_violin() +
  ggtitle("Price by Brand Distribution")
```

The following graph lays out data points by the mileage that cars have driven and the associated cost for that car, which is color-coded by its brand, we have a few initial takeaways about how our different variables might be effecting the price. Firstly, we notice an exponential decay in cost as mileage increases. Additionally, we see that Volkswagen is by far the cheapest and has a relatively small range in comparison to its counterparts. This makes sense because Volkswagen is typically not thought of as a luxury brand so we would not  expect many super high-priced vehicles. Above that, we see a sea of blue followed by a sea of green. This means that Porsche tends to be a little cheaper than Mercedes. However, we see that some new Porsches can be quite expensive while those with more miles on them tends to be a little cheaper to a comparable Mercedes.

```{r, fig.width=6, fig.height=4}
#All Graph
ggplot(all.clean,aes(x=Mileage,y=Price)) +
  geom_point(aes(color=Brand)) +
  geom_point(aes(color=Brand))+facet_wrap(~Brand) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_y_continuous(labels = dollar) +
  scale_x_continuous(labels = label_comma()) +
  ggtitle("Price by Brand and Mileage")
```

#### Audi
This plot gives us a good idea of how the price differs across the class of the car for different Audi's. We can see pretty strong right-skews for A, R, and T class cars, meaning that they likely have some very high-end cars. On the other hand we can see for e-class cars (representing etrons) that there is a left skew,meaning that these cars tend to be priced on the specific model of car. Moreso, it means that electronic cars being made by Audi tend to be priced in a small range. While, this range is higher than the middle for other car classes, we see that there are also a lot of cars on the lower end of the scale of the distribution. For A, T, Q, and S class cars, we see that their mean tends to be close to the same but we have a much larger variability for the T-class cars. We find that R-class cars have the most variability as the data is almost bimodal with two different peaks. This suggests that there are two sets of R-class cars, one whose mean is near (but still higher) than the other classes and an other that is much higher end and expense than the other classes.

```{r, fig.width=6, fig.height=4}
#Audi Plot
ggplot(audi.clean, aes(x=Price, color=Class)) +
  geom_density(size=2) +
  scale_y_continuous(labels = label_comma()) +
  scale_x_continuous(labels = dollar) +
  ggtitle("Audi: Density by Price and Class")
```

#### BMW
From this graphic, we see that x7's have the least mileage on them by far and also are representative of much of the newer cars as they have the lowest mean average age. Therefore, from the graphs we can not deem that x7's are higher priced just due to being x7's because other factors like mileage and age might also be playing into their higher price.

```{r, fig.width=6, fig.height=4}
bmwDataTable = as.data.table(bmw.clean)
bmwDataTable<-bmwDataTable[,.(Mileage =mean(Mileage), Years_Old =mean(Years_Old)), by=.(Style)]
#KEEP
ggplot(bmwDataTable, aes(x=Style, y=Mileage)) + 
  geom_point(size=3) + 
  geom_segment(aes(x=Style, 
                   xend=Style, 
                   y=0, 
                   yend=Mileage)) + 
  ggtitle("BMW: Average Mileage by Style") +
  scale_y_continuous(labels = label_comma()) +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```


#### Mercedes Benz
From the horizontal bar chart above, we get an idea of how the mean price differs by style of car in general, as well as how the impact of the car being a base model versus not a base model also impacts the price. We can see that in general the other category is by far the most expensive, followed by the g class and s class. Also, we see for S, E, and C classes, the average price of the base models is significantly lower than that of other models but for the G class and other vehicles, the Base class actually ends up being more expensive on average. This is definitely something we will want to take into account for our Mercedes-specific pricing model. We want to ensure that this is something that we want to capture in our model, so we will make sure that other variables are not impacting and skewing this data.

```{r, fig.width=6, fig.height=4}
library(ggfortify)
library(ggthemes)
library(data.table)
mercedesDataTable3 = as.data.table(mercedes.clean)
mercedesDataTable3<-mercedesDataTable3[,.(Price =mean(Price)), by=.(Drive, Style)]
ggplot(mercedesDataTable3, aes(x = Style, y = Price, fill = Drive)) +   
                              geom_bar(stat = "identity", width = .6) + 
                              coord_flip() +
                              ggtitle("Mercedes-Benz: Average Price by Drive Type and Style Class") +
                              scale_y_continuous(labels = dollar) +
                              theme(plot.title = element_text(hjust = .5))
```


#### Porsche
This treemap displays the average price of each Porsche model in relation to all other models and in general. Specifically, the map is configured such that we find that average price of each model and list each model in the map, and the larger the box is for the model, the more expensive it is on average. Therefore, we find that the 911 tends to be significantly more expensive than all other models, while the Cayenne and Macan are relatively comparable and the Cayman and Panamera are also relatively with one another, respectively. Lastly, we find that the Boxster tends to be significantly cheaper than all other models.

```{r, fig.width=6, fig.height=4}
porscheDataTable = as.data.table(porsche.clean)
#head(porsche.clean)
porscheDataTable<-porscheDataTable[,.(Price =mean(Price)), by=.(Style, Year)]
#KEEP
porscheDataTable2<-porscheDataTable[,.(Price =mean(Price)), by=.(Style)]
#porscheDataTable2 
ggplot(porscheDataTable2, aes(area = Price, fill = Style, label = Style)) +
  geom_treemap() +
  geom_treemap_text(colour = "white", place = "centre",
                    grow = TRUE) +
  ggtitle("Porsche: Treemap of Price by Model Style")
```


#### Volkswagen
We can clearly see that there are large differences in price across the different German brands. In order to learn more about each brand, we decided to perform price analysis within every brand. The graph below details how price differs across style of Volkswagen. Specifically, for different prices on the x-axis, we see the count of how many of each model lies within that price range. Therefore, we can see right away that Atlas' tend to lie more on the pricier side as they have greater area when the price extends past \$40,000. We can also see a large area for both Jettas and Tiguans in the middle price range of about \$20,000-30,000. We also see the Golf and Touareg lie in this range. Meanwhile, we see both the Golf and Beetle fall in bottom third of the price range at less than \$20,000 which is where a significant amount of their area lies. Thus, there seems to be a strong association with price by style (model) of car for Volkswagens. It is also worth noting that some Touarges seem to be on the very price side around \$60,000 which might skew its specific data a bit.

```{r, fig.width=6, fig.height=4}
#KEEP
theme_set(theme_classic())
ggplot(volkswagen.clean, aes(Price)) +
  geom_histogram(aes(fill=Style), 
                   binwidth = 1000, 
                   col="black") +
  scale_x_continuous(labels = dollar) +
  ggtitle("Volkswagen: Count of Price by Model Style")
```


## Modeling Techniques and Results

To begin our modeling process, the first thing we did was created two models with the full dataset: one model including all of the predictors, and one model including all of the predictors as well as interaction terms. We see that the model with interactions tends to perform better than the one with all variables fitted. This is seen in multiple ways: 1. The anova test rejects the non-interaction model in favor of the interaction model 2. The p-values of all predictors in the interaction model show a statistical significance for all variables 3. The R^2 and Adjusted R^2 increase for the interaction term model over the model without interaction terms. 

```{r warn = -1}
# Fit full model with all predictors 
all.model = lm(Price ~Years_Old + Brand + Mileage, data=all.clean)
print("Simple Model:")
summary(all.model)

# Fit full model with interaction
all.model.interactions = lm(Price ~Years_Old + Brand + Mileage + Mileage*Years_Old + Brand*Mileage, data=all.clean)
print("Interaction Model:")
summary(all.model.interactions)
print("Anova Test:")
anova(lm(Price ~Years_Old + Brand + Mileage + Mileage*Years_Old + Brand*Mileage, data=all.clean))
```


The model with interaction terms had a better R-squared value (0.4554 v. 0.3881), so we began model diagnostics on this model. The first step here was to check high leverage points. We see that we have about 1,102 out of our 15,000 observations marked as high leverage points. That means that nearly 6% of our data is considered to have some predictor have an extreme value which can have large effects on our modeling efforts being very accurate. Out of which only about 189 are considered bad leverage point which is only about 1% of the data, making it seem that our model will be more accurate then we may have initially thought.

```{r warn = -1}
# Unusual Observations

# Leverage Points
all.leverages = lm.influence(all.model.interactions)$hat
#head(all.leverages)
library(faraway)
halfnorm(all.leverages, nlab=6, labs=as.character(1:length(all.leverages)), ylab="Leverages")
#summary(all.model.interactions)

n = dim(all.clean[,c(-1,-16)])[1]; # Sample size
#n

p = length(variable.names(all.model.interactions))
#p

all.model.interactions.leverages.high = all.leverages[all.leverages>2*p/n]
print("High Leverage Points:")
length(all.model.interactions.leverages.high)

# Calculate the IQR for the dependent variable 
IQR_y = IQR(all.clean$Price)

#Define a range with its lower limit being (Q1 - IQR) and upper limit being (Q3 + IQR) 
QT1_y = quantile(all.clean$Price,0.25)
QT3_y = quantile(all.clean$Price,0.75)

lower_lim_y = QT1_y - IQR_y
upper_lim_y = QT3_y + IQR_y

vector_lim_y = c(lower_lim_y,upper_lim_y)

# Range for y variable 
#vector_lim_y

# Extract observations with high leverage points from the original data frame 
all.highlev = all.clean[,c(-1,-16)][all.leverages>2*p/n,]

# Select only the observations with leverage points outside the range 
all.highlev_lower = all.highlev[all.highlev$Price < vector_lim_y[1], ]
all.highlev_upper = all.highlev[all.highlev$Price > vector_lim_y[2], ]
all.highlev = rbind(all.highlev_lower,all.highlev_upper)
print("Bad Leverage Points:")
nrow(all.highlev)

# Outliers
# Computing Studentized Residuals #
all.resid = rstudent(all.model.interactions); 

# Critical value WITH Bonferroni correction #
bonferroni_cv = qt(.05/(2*n), n-p-1) 
#bonferroni_cv
```


Additionally, we see that our data only outputs about 10 outliers which is an extremely small percentage of the data and is to be expected. While we would like to remove those points from our model and it would make sense (since they might be the multi-hundred thousand dollar Porsche's that our predictors can't account for), we unfortunately can not without consulting with an industry professional. Still, with only such a small percentage of observations being outliers we do not expect them to have too big of an effect on the accuracy of our model.

```{r }
# Sorting the residuals in descending order to find outliers (if any) 
all.resid.sorted = sort(abs(all.resid), decreasing=TRUE)[1:10]
birthweight.outliers = all.resid.sorted[abs(all.resid.sorted) > abs(bonferroni_cv)]
print("Number of Outliers:")
length(birthweight.outliers)
```

We also see that we have no influential points which is another good sign that none of the outliers appear to have a significant impact on the regression model that we have fitted, and we feel confident moving towards additional steps of the model selection process.

```{r warn = -1}
# Influential Points
all.cooks = cooks.distance(all.model.interactions)
print("Influential Points:")
length(all.cooks[all.cooks] > 1)
```


We see that the constant variance assumption fails as the variance in error stands to increase and has a fan shape, additionally we fail the BP test. This heteroskedacity must be fixed because otherwise we will obtain p-values smaller than what they are in reality, resulting in us having a model with predictors we do not need, and, thus, losing its simplicity.

```{r, fig.width=6, fig.height=4}
bptest(all.model.interactions)
plot(all.model.interactions, which=1)
```

We also see that our model currently fails the normality assumption. This will again effect the values of our p-values across our variable predictors when we test them and may deviate our results from their true meaning, unless fixed. Therefore, we will try to turn to transformations to fix these problems.

```{r warn = -1}
# Normality
plot(all.model.interactions, which=2) #normality assumption failed
```


Since the optimal lambda from the box-cox transformation was very close to zero, we decided to do a log transformation on our y-variable (Price). While the new model still fails the tests, we still see great improvement in the graphs as the variance in error appears to be relatively constant and many points on the QQ-plot fit along the linear line. Additionally, we see that our Adjusted R^2 also improves a significant amount. Therefore, we will continue to use this model with the transformed y through the rest of this study.
```{r warn = -1}
# Box-Cox Transformation
library(MASS)
#all.transformation <- boxcox(all.model.interactions, lambda=seq(-2, 2, length=400))
#(lambda <- all.transformation$x[which.max(all.transformation$y)])

all.clean2 = all.clean
all.clean2$Price.new = log(all.clean2$Price)
all.model.transformed = lm(Price.new ~ Years_Old + Brand + Mileage + Mileage*Years_Old + Brand*Mileage, data=all.clean2)
#summary(all.model.transformed)
```

```{r, fig.width=6, fig.height=4}
plot(all.model.transformed, which=2)
```

```{r fig.width=6, fig.height=4}
# Constant Variance
plot(all.model.transformed, which=1)
```

The final model that we are choosing is the one outputted after running the BIC criterion. Even though Volkswagen is not significant on its own, its interaction terms are, so we will keep it in our model as well. Therefore, our final model is the log price which is equal to the coefficients listed in the table above. We find that this model has an only slightly worse error for our training and testing dataset. Additionally, this model removed ones that were found insignificant in our original model, so we feel confident with this model describing the log price of all of our data across the different brands, ages of cars, and mileage of cars. We will perform similar analysis below on each specific brand dataset individually as well.
```{r warn = -1}
# Variable Selection

# Testing and Training Sets
set.seed(447)
smp_size <- floor(0.5 * nrow(all.clean2))
train_ind <- sample(seq_len(nrow(all.clean2)), size = smp_size)
train <- all.clean2[train_ind, ]
test <- all.clean2[-train_ind, ]

library(leaps)
#head(all.clean2)
regsubsets_selection=regsubsets(Price.new ~ Years_Old + Brand + Mileage + Mileage*Years_Old + Brand*Mileage, data = train, nvmax=15)
rs = summary(regsubsets_selection)
#rs$adjr2
#which.max(rs$adjr2)
#rs$bic
#which.min(rs$bic)
#rs$which

myfit.adjr2 = lm(Price.new ~ Years_Old + Brand + Mileage + Mileage*Years_Old + Brand*Mileage, data=train)
#summary(myfit.adjr2)

rmse<-function(x,y) sqrt(mean((x-y)^2))

print("Final Model:")
myfit.bic = update(myfit.adjr2, Price.new ~ . -BMW -BMW*Mileage -Mercedes-Benz*Mileage, data=train)
summary(myfit.bic)
# Training Error
print("Training Error:")
rmse(fitted(myfit.bic), train$Price.new)
# Testing Error
print("Testing Error:")
rmse(predict(myfit.bic, test), test$Price.new)
```

#### Audi Model
We actually see that the model fit improves only slightly with the addition of all predictors and interaction terms (Adjusted R^2 increases from .74 to .78). Again, we significantly fail both the normality and constant variance tests and move to a linear transformation. Again, we see that a box-cox transformation with a log transformation of the y improves these two issues. Additionally, we see that the BIC model has almost the exact same training and testing errors and has a slightly less adjusted R^2 at .89 versus .9. Therefore, our final model will be the one produced by our BIC criterion above. We will follow a similar  process for the rest of our models and shortcut some steps and explanation unless we see some different statistical results that deviate from the pattern we have seen thus far.
```{r warn = -1}
# Correlation
#head(audi.clean)
#cor(audi.clean[,c("Years_Old","Mileage", "Price")])
print("Simple Model:")
audi.model = lm(Price ~Years_Old + Mileage + Class + Type, data=audi.clean)
summary(audi.model)
print("Interaction Model:")
audi.model.interactions = lm(Price ~Years_Old + Mileage + Class + Type + Years_Old*Class + Years_Old*Class + Years_Old*Type + Mileage*Class + Mileage*Type + Class*Type, data=audi.clean)
summary(audi.model.interactions)
print("Anova Test:")
anova(lm(Price ~Years_Old + Mileage + Class + Type + Years_Old*Class + Years_Old*Class + Years_Old*Type + Mileage*Class + Mileage*Type + Class*Type, data=audi.clean))

# New Model Assumptions
print("Variance Assumption:")
plot(audi.model.interactions, which=1)
bptest(audi.model.interactions)

# Normality
print("Normality Assumption:")
plot(audi.model.interactions, which=2)
ks.test(audi.model.interactions$residuals, "pnorm")

audi.clean2 = audi.clean
audi.clean2$Price.new = log(audi.clean2$Price)
print("Transformed Model:")
audi.model.transformed = lm(Price.new ~ Years_Old + Mileage + Class + Type + Years_Old*Class + Years_Old*Class + Years_Old*Type + Mileage*Class + Mileage*Type + Class*Type, data=audi.clean2)
summary(audi.model.transformed)


# New Model Assumptions
print("Transformed Model: Variance Assumption:")
plot(audi.model.transformed, which=1)
bptest(audi.model.transformed)

# Normality
print("Transformed Model: Normality Assumption:")
plot(audi.model.transformed, which=2)
ks.test(audi.model.transformed$residuals, "pnorm")

# Testing and Training Sets
set.seed(447)
smp_size <- floor(0.5 * nrow(audi.clean2))
train_ind <- sample(seq_len(nrow(audi.clean2)), size = smp_size)
train <- audi.clean2[train_ind, ]
test <- audi.clean2[-train_ind, ]

library(leaps)
regsubsets_selection=regsubsets(Price.new ~ Years_Old + Mileage + Class + Type + Years_Old*Class + Years_Old*Class + Years_Old*Type + Mileage*Class + Mileage*Type + Class*Type, data = train, nvmax=45)
rs = summary(regsubsets_selection)
#rs$adjr2
#which.max(rs$adjr2)
#rs$bic
#which.min(rs$bic)
#rs$which[26, ]
#rs$which[17, ]

myfit.adjr2 = update(audi.model.interactions, Price.new ~ . -Q -Years_Old:ClassE -Years_Old:ClassS -Mileage:ClassE -Mileage:ClassQ -Mileage:ClassT -Mileage:TypePremium_Plus -ClassE:TypePremium -ClassR:TypePremium -ClassS:TypePremium -ClassT:TypePremium -ClassE:TypePremium_Plus -ClassR:TypePremium_Plus -ClassS:TypePremium_Plus -ClassT:TypePrestige, data=train)
#summary(myfit.adjr2)

rmse<-function(x,y) sqrt(mean((x-y)^2))

print("Final Model:")
myfit.bic = update(audi.model.interactions, Price.new ~ . -T -Years_Old:ClassE -Years_Old:ClassS -Mileage:ClassE -Mileage:ClassQ -Mileage:ClassS -Mileage:ClassT -Mileage:TypePremium -Mileage:TypePremium_Plus -Mileage:TypePrestige -ClassE:TypePremium -ClassR:TypePremium -ClassS:TypePremium -ClassT:TypePremium -ClassE:TypePremium_Plus -ClassQ:TypePremium_Plus -ClassR:TypePremium_Plus -ClassS:TypePremium_Plus -ClassT:TypePremium_Plus -ClassE:TypePrestige -ClassQ:TypePrestige -ClassR:TypePrestige -ClassS:TypePrestige -ClassT:TypePrestige, data=train)
summary(myfit.bic)
# Training Error
print("Training Error:")
rmse(fitted(myfit.bic), train$Price.new)
# Testing Error
print("Testing Error:")
rmse(predict(myfit.bic, test), test$Price.new)
```

#### BMW Model

We see similar model selection trends for BMW, Mercedes, Porsche, and Volkswagen, and, thus, choose the same BIC model with the y transformed by log for all of these brand-specific models as well. Continually, we see this BIC model perform quite well with a high adjusted R^2, fewer used variables, and a low Root Mean Squared Error. The rest of the models (and their diagnostics) continue and are labeled as such. Overall, we feel confident with using these models in our data analysis. 
```{r warn = -1}
print("Simple Model:")
bmw.model = lm(Price ~Years_Old + Mileage + Style + Type, data=bmw.clean)
summary(bmw.clean)

print("Interaction Model:")
bmw.model.interactions = lm(Price ~Years_Old + Mileage + Style + Type + Years_Old*Style + Years_Old*Style + Years_Old*Type + Mileage*Style + Mileage*Type + Style*Type, data=bmw.clean)
summary(bmw.model.interactions)

print("Variance Assumption:")
plot(bmw.model.interactions, which=1)

print("Normality Assumption:")
plot(bmw.model.interactions, which=2)

# Box-Cox Transformation
#bmw.transformation = boxcox(bmw.model.interactions, lambda=seq(-2, 2, length=400))
#(lambda <- all.transformation$x[which.max(all.transformation$y)])


bmw.clean2 = bmw.clean
bmw.clean2$Price.new = log(bmw.clean2$Price)
print("Transformed Model:")
bmw.model.transformed = lm(Price.new ~ Years_Old + Mileage + Style + Type + Years_Old*Style + Years_Old*Style + Years_Old*Type + Mileage*Style + Mileage*Type + Style*Type, data=bmw.clean2)
summary(bmw.model.transformed)

# New Model Assumptions
print("Transformed Model: Variance Assumption:")
plot(bmw.model.transformed, which=1)

print("Transformed Model: Normality Assumption:")
plot(bmw.model.transformed, which=2)

# Testing and Training Sets
set.seed(447)
smp_size <- floor(0.5 * nrow(bmw.clean2))
train_ind <- sample(seq_len(nrow(bmw.clean2)), size = smp_size)
train <- bmw.clean2[train_ind, ]
test <- bmw.clean2[-train_ind, ]

library(leaps)
regsubsets_selection=regsubsets(Price.new ~ Years_Old + Mileage + Style + Type + Years_Old*Style + Years_Old*Style + Years_Old*Type + Mileage*Style + Mileage*Type + Style*Type, data = train, nvmax=45)
rs = summary(regsubsets_selection)
#rs$bic
#which.min(rs$bic)
#rs$which[16, ]

print("Final Model:")
myfit.bic = update(bmw.model.interactions, Price.new ~ . -T -Years_Old:ClassE -Years_Old:ClassS -Mileage:ClassE -Mileage:ClassQ -Mileage:ClassS -Mileage:ClassT -Mileage:TypePremium -Mileage:TypePremium_Plus -Mileage:TypePrestige -ClassE:TypePremium -ClassR:TypePremium -ClassS:TypePremium -ClassT:TypePremium -ClassE:TypePremium_Plus -ClassQ:TypePremium_Plus -ClassR:TypePremium_Plus -ClassS:TypePremium_Plus -ClassT:TypePremium_Plus -ClassE:TypePrestige -ClassQ:TypePrestige -ClassR:TypePrestige -ClassS:TypePrestige -ClassT:TypePrestige, data=train)
summary(myfit.bic)

# Training Error
print("Training Error:")
rmse(fitted(myfit.bic), train$Price.new)
# Testing Error
print("Testing Error:")
rmse(predict(myfit.bic, test), test$Price.new)
```

#### Mercedes Benz Model

```{r warn = -1}
print("Simple Model:")
mercedes.model = lm(Price ~Years_Old + Mileage + Style + Type + Drive, data=mercedes.clean)
summary(mercedes.clean)

print("Interaction Model:")
mercedes.model.interactions = lm(Price ~Years_Old + Mileage + Style + Type + Drive + Years_Old*Style + Years_Old*Style + Years_Old*Type + Mileage*Style + Mileage*Type + Style*Type + Drive*Years_Old + Drive*Mileage + Drive*Style + Drive*Type, data=mercedes.clean)
summary(mercedes.model.interactions)

print("Variance Assumption:")
plot(mercedes.model.interactions, which=1)

print("Normality Assumption:")
plot(mercedes.model.interactions, which=2)

# Box-Cox Transformation
#mercedes.transformation = boxcox(mercedes.model.interactions, lambda=seq(-2, 2, length=400))
#(lambda <- all.transformation$x[which.max(all.transformation$y)])

print("Transformed Model:")
mercedes.clean2 = mercedes.clean
mercedes.clean2$Price.new = log(mercedes.clean2$Price)
mercedes.model.transformed = lm(Price.new ~ Years_Old + Mileage + Style + Type + Drive + Years_Old*Style + Years_Old*Style + Years_Old*Type + Mileage*Style + Mileage*Type + Style*Type + Drive*Years_Old + Drive*Mileage + Drive*Style + Drive*Type, data=mercedes.clean2)
summary(mercedes.model.transformed)

# New Model Assumptions
print("Transformed Model: Variance Assumption:")
plot(mercedes.model.transformed, which=1)

print("Transformed Model: Normality Assumption:")
plot(mercedes.model.transformed, which=2)

# Testing and Training Sets
set.seed(447)
smp_size <- floor(0.5 * nrow(mercedes.clean2))
train_ind <- sample(seq_len(nrow(mercedes.clean2)), size = smp_size)
train <- mercedes.clean2[train_ind, ]
test <- mercedes.clean2[-train_ind, ]

library(leaps)
regsubsets_selection=regsubsets(Price.new ~ Years_Old + Mileage + Style + Type + Drive + Years_Old*Style + Years_Old*Style + Years_Old*Type + Mileage*Style + Mileage*Type + Style*Type + Drive*Years_Old + Drive*Mileage + Drive*Style + Drive*Type, data = train, nvmax=45)
rs = summary(regsubsets_selection)
#rs$bic
#which.min(rs$bic)
#rs$which[17, ]

print("Final Model:")
myfit.bic = update(mercedes.model.interactions, Price.new ~ . -TypeOther -Years_Old:StyleOther, data=train)
summary(myfit.bic)

# Training Error
print("Training Error:")
rmse(fitted(myfit.bic), train$Price.new)
# Testing Error
print("Testing Error:")
rmse(predict(myfit.bic, test), test$Price.new)
```

#### Porsche Model
```{r warn = -1}
print("Simple Model:")
porsche.model = lm(Price ~Years_Old + Mileage + Style, data=porsche.clean)
summary(porsche.clean)

print("Interaction Model:")
porsche.model.interactions = lm(Price ~Years_Old + Mileage + Style + Years_Old*Style + Years_Old*Style + Mileage*Style, data=porsche.clean)
summary(porsche.model.interactions)

print("Variance Assumption:")
plot(porsche.model.interactions, which=1)

print("Normality Assumption:")
plot(porsche.model.interactions, which=2)

# Box-Cox Transformation
#porsche.transformation = boxcox(porsche.model.interactions, lambda=seq(-2, 2, length=400))
#(lambda <- all.transformation$x[which.max(all.transformation$y)])

print("Transformed Model:")
porsche.clean2 = porsche.clean
porsche.clean2$Price.new = log(porsche.clean2$Price)
porsche.model.transformed = lm(Price.new ~ Years_Old + Mileage + Style + Years_Old*Style + Years_Old*Style + Mileage*Style, data=porsche.clean2)
summary(porsche.model.transformed)

# New Model Assumptions
print("Transformed Model: Variance Assumption:")
plot(porsche.model.transformed, which=1)

print("Transformed Model: Normality Assumption:")
plot(porsche.model.transformed, which=2)

# Testing and Training Sets
set.seed(447)
smp_size <- floor(0.5 * nrow(mercedes.clean2))
train_ind <- sample(seq_len(nrow(mercedes.clean2)), size = smp_size)
train <- mercedes.clean2[train_ind, ]
test <- mercedes.clean2[-train_ind, ]

library(leaps)
set.seed(447)
smp_size <- floor(0.5 * nrow(porsche.clean2))
train_ind <- sample(seq_len(nrow(porsche.clean2)), size = smp_size)
train <- porsche.clean2[train_ind, ]
test <- porsche.clean2[-train_ind, ]
#rs$bic
#which.min(rs$bic)
#rs$which[12, ]

print("Final Model:")
myfit.bic = update(porsche.model.interactions, Price.new ~ . -Years_Old:StyleMacan -Years_Old:StyleOther -Mileage:StyleBoxster -Mileage:StyleCayenne -Mileage:StyleCayman -Mileage:StyleMacan -Mileage:StyleOther -Mileage:StylePanamera , data=train)
summary(myfit.bic)

# Training Error
print("Training Error:")
rmse(fitted(myfit.bic), train$Price.new)
# Testing Error
print("Testing Error:")
rmse(predict(myfit.bic, test), test$Price.new)
```

#### Volkswagen Model
```{r warn = -1}
print("Simple Model:")
volkswagen.model = lm(Price ~Years_Old + Mileage + Style, data=volkswagen.clean)
summary(volkswagen.clean)

print("Interaction Model:")
volkswagen.model.interactions = lm(Price ~Years_Old + Mileage + Style + Years_Old*Style + Years_Old*Style + Mileage*Style, data=volkswagen.clean)
summary(volkswagen.model.interactions)

print("Variance Assumption:")
plot(volkswagen.model.interactions, which=1)

print("Normality Assumption:")
plot(volkswagen.model.interactions, which=2)

# Box-Cox Transformation
#mercedes.transformation = boxcox(mercedes.model.interactions, lambda=seq(-2, 2, length=400))
#(lambda <- all.transformation$x[which.max(all.transformation$y)])

print("Transformed Model:")
volkswagen.clean2 = volkswagen.clean
volkswagen.clean2$Price.new = log(volkswagen.clean2$Price)
volkswagen.model.transformed = lm(Price.new ~ Years_Old + Mileage + Style + Years_Old*Style + Years_Old*Style + Mileage*Style, data=volkswagen.clean2)
summary(volkswagen.model.transformed)

# New Model Assumptions
print("Transformed Model: Variance Assumption:")
plot(volkswagen.model.transformed, which=1)

print("Transformed Model: Normality Assumption:")
plot(volkswagen.model.transformed, which=2)

# Testing and Training Sets
set.seed(447)
smp_size <- floor(0.5 * nrow(volkswagen.clean2))
train_ind <- sample(seq_len(nrow(volkswagen.clean2)), size = smp_size)
train <- volkswagen.clean2[train_ind, ]
test <- volkswagen.clean2[-train_ind, ]

library(leaps)
regsubsets_selection=regsubsets(Price.new ~ Years_Old + Mileage + Style + Years_Old*Style + Years_Old*Style + Mileage*Style, data = train, nvmax=45)
rs = summary(regsubsets_selection)
#rs$bic
#which.min(rs$bic)
#rs$which[15, ]

print("Final Model:")
myfit.bic = update(volkswagen.model.interactions, Price.new ~ . -Years_Old:StyleGolf -Years_Old:StyleJetta -Years_Old:StylePassat -Years_Old:StyleTiguan -Mileage:StyleGolf -Mileage:StyleJetta -Mileage:StylePassat -Mileage:StyleTiguan, data=train)
summary(myfit.bic)

# Training Error
print("Training Error:")
rmse(fitted(myfit.bic), train$Price.new)
# Testing Error
print("Testing Error:")
rmse(predict(myfit.bic, test), test$Price.new)
```


# Conclusion
Overall, we were able to learn a lot through our dataset about the different factors that effect the price of German cars and how we can apply that to our own personal needs and life. Specifically, one of the most important things that we saw was the agreement between our models, visualizations, and data analysis. By seeing interactions that did not look significant in a graph also have unsignificance in our final models made us quite confident about ou models ability to help us solve our problem. The creation of the different model for each brand and model with all brands combined made for great comparisons as well. We were able to see that the model selection process was almost the same, in terms of the use of a log transformation on the y and other changes to make the model stronger and more fortified. We were able to learn about the correlation between age and mileage on cars across different brands and how for specific brands price is effected by brand-specific variables. For example, with Porsche's the model typically effects the cars age and its pirce while for Volkswagen's we have two segments of data based on the age of the Volkswagen which shows that there might exist more vintage VW's adn newer ones, which are priced quite similarly. 

Overall, we were able to learn about how different variables such as class, model, age, drive type, and mileage and their interactions effect the price of cars in general and across our different brands. We look forward to applying this model to our own personal needs as we shop around for some of these fast, luxorious German cars.

## Statistical Conclusions
AJ: I personally am very interested in high-end German cars (specifically from Porsche or BMW, but obviously want the car to be cheap (less than $45,000). Furthermore, I want the car to be a sedan. I do not really care about the age or mileage on the car however. By using our model, I was able to find a set of cars that fit this mold and were likely significantly cheaper than an other model I would get. This has significant impacts for me as I am able to narrow down a potential car search and still afford a luxorious car that I can enjoy driving.
Andrew: I felt like there was a lot to take away from this project, and the most interesting thing that I will take away is that data science programming methods can save someone a lot of money. Developing a model to identify the prices of luxury German cars that have the specific mileage, age, and brand of your choosing can help someone identify what cars could be over or under-priced and where one can save money. Personally, I have a new respect for data science after this project, and I will try to embrace this analytical-process in future queries.
Mai: I found it fascinating that some luxury brands actually had cheaper cars that seemed inherently better based off of our model and data. For example, I found some 
newer Porsche's with little mileage on them that were actually singificantly cheaper than older Volkswagen's with some decent mileage on them. It really makes me
reevaluate how car's can be priced and gives more insight into how I would choose a potential German car. 
Jamie: I was amazed by the outputs of our modeling. I did not expect all of the interaction terms to be significant. I was also super surprised that a lot of the variables that we manual created from the initial dataset ended up having great significance. It goes to show the importance of evaluating your data and observing it properly prior to hopping into statistical analysis right away.

## Personal Takeaways
AJ: Realizing the power of combining topics across different STAT courses. Specifically, I was able to take a lot from STAT 425 (Statistical Modeling) and use it in conjunction with topics from this course. For example, prior to creating our statistical models, which largely used topics I learned from STAT 425, I had to clean and evaluate the data, as well as create visualizations to better help me understand what the data looked like and where relationships may exist. This allowed me to create better models with the creation of new variables and removal of unnecessary ones that would not have been caught by ordinary model testing but was found through the initial analysis and visualizations of our data.
Andrew: Through this project and this class, I believe that I was able to grow as a data engineer, data scientist, and data analyst. We started to start the class with the basics of shell programming in order to understand the basics of data systems and structures. We used the knowledge and experience from this unit to retrieve and store the data correctly so that everyone in our group could work with it. In addition to this, I felt like the data manipulation skills that we learned in this class were crucial for our success in this project. From this project, I feel like I have been able to implement the skills that we learned in class and practiced in assignments and quizzes to ultimately create a complete analysis of something that I am passionate about.
Mai: I loved seeing how the capabilities of R compared to those of Python. I had always used Python for statistical analysis and building things like web scrapers, so I was amazed by how easy it was to implement similar functionalities in R. I am starting to see the power of R as a statistical language more and more.
Jamie: I could not believe how much I was able to grow and learn from this project. I had no prior experience in Shiny applications or anything related to it, but I was able to take just a small lecture about it and create some pretty cool application that has some actual use applications. It goes to show how you can just take some small pieces of a course like this and really build on it at deeper levels.
